# ğŸ§  Medical Text Summarization Model  

> A deep learning project for **abstractive summarization of medical literature and systematic reviews**, built with Transformers and trained on PubMed & MSÂ² datasets.  

---

## ğŸ“– Overview  

This repository contains a complete workflow for training, evaluating, and deploying a **Transformer-based summarization model** for the biomedical domain. The goal is to generate **concise, accurate summaries of complex medical documents** such as clinical studies, PubMed abstracts, and systematic reviewsâ€”helping researchers, clinicians, and students digest critical information faster.  

---

## âœ¨ Key Features  

- âš™ï¸ **Custom GPT-style decoder** built from scratch with PyTorch  
- ğŸ“š **Domain-specific datasets**: PubMed & MSÂ² reviews  
- ğŸ” **End-to-end pipeline**: preprocessing â†’ tokenization â†’ training â†’ evaluation â†’ inference  
- ğŸ“Š **Evaluation metrics**: BLEU, ROUGE, F1  
- ğŸ’¾ **Pre-trained checkpoints** (`gpt_decoder_epoch*.pt`) for reproducibility  
- ğŸ““ **Interactive Jupyter notebook** (`text_summ_model_training.ipynb`) for experimentation  
- ğŸ“‘ **Comprehensive documentation** included in PDF  

---

## ğŸ—‚ï¸ Repository Structure  

MedicalTextSummarizationModel/
â”‚
â”œâ”€â”€ Dataset.zip # Original dataset bundle
â”œâ”€â”€ text_summ_model_training.ipynb # Main training & evaluation notebook
â”œâ”€â”€ gpt_decoder_epoch1.pt # Saved model checkpoint
â”œâ”€â”€ gpt_decoder_epoch2.pt
â”œâ”€â”€ gpt_decoder_epoch3.pt
â”œâ”€â”€ Medical_Txt_Summarization_Model_Documentation.pdf
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ readme.txt # (legacy notes)

yaml
Copy code

---

## ğŸš€ Getting Started  

### 1. Clone the repo  
```bash
git clone https://github.com/Fadyeskand224/MedicalTextSummarizationModel.git
cd MedicalTextSummarizationModel
2. Install dependencies
bash
Copy code
python3 -m venv .venv
source .venv/bin/activate   # on macOS/Linux
# .venv\Scripts\activate    # on Windows
pip install -r requirements.txt
3. Download datasets
Unzip the dataset bundle:

bash
Copy code
unzip Dataset.zip -d Dataset/
4. Train the model
Open the notebook:

bash
Copy code
jupyter notebook text_summ_model_training.ipynb
Follow the cells to preprocess data, train, and evaluate.

ğŸ“Š Results
Training: 3 epochs on PubMed/MSÂ² data

Metrics: ROUGE-1, ROUGE-2, ROUGE-L, BLEU

Findings: Model achieves strong abstractive summaries compared to baseline methods

ğŸ“¦ Model Checkpoints
gpt_decoder_epoch1.pt â†’ baseline training run

gpt_decoder_epoch2.pt â†’ improved performance with longer training

gpt_decoder_epoch3.pt â†’ final, best performing checkpoint

ğŸ”® Future Work
Fine-tune with Pegasus and BERTSUM baselines for comparison

Extend evaluation with clinical notes datasets (e.g., MIMIC-III)

Deploy as a REST API for real-time summarization

ğŸ¤ Contributing
Contributions are welcome! Fork the repo, create a branch, and open a pull request with improvements.

ğŸ™Œ Acknowledgements
PubMed Summarization Dataset

MSÂ² Dataset

Hugging Face Transformers

PyTorch
